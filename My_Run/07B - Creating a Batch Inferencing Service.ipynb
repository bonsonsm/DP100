{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating a Batch Inferencing Service\n",
    "\n",
    "In previous labs, you used an Azure ML *pipeline* to automate the training and registration of a model, and you published a model as a web service for real-time *inferencing* (getting predictions from a model). Now you'll combine these two concepts to create a pipeline for *batch inferencing*. What does that mean? Well, imagine a health clinic takes patient measurements all day, saving the details for each patient in a separate file. Then overnight, the diabetes prediction model can be used to process all of the day's patient data as a batch, generating predictions that will be waiting the following morning so that the clinic can follow up with patients who are predicted to be at risk of diabetes. That's what you'll implement in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Your Workspace\n",
    "\n",
    "First, connect to your workspace using the Azure ML SDK.\n",
    "\n",
    "> **Note**: If the authenticated session with your Azure subscription has expired since you completed the previous exercise, you'll be prompted to reauthenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.13.0 to work with BSM_MLWorkspace3\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Register a Model\n",
    "\n",
    "If you have completed the previous lab in this course, you will have registered many versions of the **diabetes_model** model, and you're ready to go.\n",
    "\n",
    "If you haven't, you can run the cell below to train and register the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment: diabetes-training\n",
      "Loading Data...\n",
      "Training a decision tree model\n",
      "Accuracy: 0.8883333333333333\n",
      "AUC: 0.8760908502910593\n",
      "Model trained and registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.core import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Create an Azure ML experiment in your workspace\n",
    "experiment = Experiment(workspace = ws, name = \"diabetes-training\")\n",
    "run = experiment.start_logging()\n",
    "print(\"Starting experiment:\", experiment.name)\n",
    "\n",
    "# load the diabetes dataset\n",
    "print(\"Loading Data...\")\n",
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a decision tree model\n",
    "print('Training a decision tree model')\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# Save the trained model\n",
    "model_file = 'diabetes_model.pkl'\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "run.upload_file(name = 'outputs/' + model_file, path_or_stream = './' + model_file)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()\n",
    "\n",
    "# Register the model\n",
    "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
    "                   tags={'Training context':'Inline Training'},\n",
    "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
    "\n",
    "print('Model trained and registered.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Upload Batch Data\n",
    "\n",
    "Since we don't actually have a fully staffed clinic with patients from whom to get new data for this course, you'll generate a random sample from our diabetes CSV file and use those to test the pipeline. Then you'll upload that data to a datastore in the Azure Machine Learning workspace and register a dataset for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aml_data - Default = False\n",
      "workspaceblobstore - Default = True\n",
      "workspacefilestore - Default = False\n",
      "Folder created!\n",
      "Saving files...\n",
      "files saved!\n",
      "Uploading files to datastore...\n",
      "Uploading an estimated of 100 files\n",
      "Uploading batch-data/1.csv\n",
      "Uploaded batch-data/1.csv, 1 files out of an estimated total of 100\n",
      "Uploading batch-data/10.csv\n",
      "Uploaded batch-data/10.csv, 2 files out of an estimated total of 100\n",
      "Uploading batch-data/100.csv\n",
      "Uploaded batch-data/100.csv, 3 files out of an estimated total of 100\n",
      "Uploading batch-data/11.csv\n",
      "Uploaded batch-data/11.csv, 4 files out of an estimated total of 100\n",
      "Uploading batch-data/12.csv\n",
      "Uploaded batch-data/12.csv, 5 files out of an estimated total of 100\n",
      "Uploading batch-data/13.csv\n",
      "Uploaded batch-data/13.csv, 6 files out of an estimated total of 100\n",
      "Uploading batch-data/14.csv\n",
      "Uploaded batch-data/14.csv, 7 files out of an estimated total of 100\n",
      "Uploading batch-data/15.csv\n",
      "Uploaded batch-data/15.csv, 8 files out of an estimated total of 100\n",
      "Uploading batch-data/16.csv\n",
      "Uploaded batch-data/16.csv, 9 files out of an estimated total of 100\n",
      "Uploading batch-data/17.csv\n",
      "Uploaded batch-data/17.csv, 10 files out of an estimated total of 100\n",
      "Uploading batch-data/18.csv\n",
      "Uploaded batch-data/18.csv, 11 files out of an estimated total of 100\n",
      "Uploading batch-data/19.csv\n",
      "Uploaded batch-data/19.csv, 12 files out of an estimated total of 100\n",
      "Uploading batch-data/2.csv\n",
      "Uploaded batch-data/2.csv, 13 files out of an estimated total of 100\n",
      "Uploading batch-data/20.csv\n",
      "Uploaded batch-data/20.csv, 14 files out of an estimated total of 100\n",
      "Uploading batch-data/21.csv\n",
      "Uploaded batch-data/21.csv, 15 files out of an estimated total of 100\n",
      "Uploading batch-data/22.csv\n",
      "Uploaded batch-data/22.csv, 16 files out of an estimated total of 100\n",
      "Uploading batch-data/23.csv\n",
      "Uploaded batch-data/23.csv, 17 files out of an estimated total of 100\n",
      "Uploading batch-data/24.csv\n",
      "Uploaded batch-data/24.csv, 18 files out of an estimated total of 100\n",
      "Uploading batch-data/25.csv\n",
      "Uploaded batch-data/25.csv, 19 files out of an estimated total of 100\n",
      "Uploading batch-data/26.csv\n",
      "Uploaded batch-data/26.csv, 20 files out of an estimated total of 100\n",
      "Uploading batch-data/27.csv\n",
      "Uploaded batch-data/27.csv, 21 files out of an estimated total of 100\n",
      "Uploading batch-data/28.csv\n",
      "Uploaded batch-data/28.csv, 22 files out of an estimated total of 100\n",
      "Uploading batch-data/29.csv\n",
      "Uploaded batch-data/29.csv, 23 files out of an estimated total of 100\n",
      "Uploading batch-data/3.csv\n",
      "Uploaded batch-data/3.csv, 24 files out of an estimated total of 100\n",
      "Uploading batch-data/30.csv\n",
      "Uploaded batch-data/30.csv, 25 files out of an estimated total of 100\n",
      "Uploading batch-data/31.csv\n",
      "Uploaded batch-data/31.csv, 26 files out of an estimated total of 100\n",
      "Uploading batch-data/32.csv\n",
      "Uploaded batch-data/32.csv, 27 files out of an estimated total of 100\n",
      "Uploading batch-data/33.csv\n",
      "Uploaded batch-data/33.csv, 28 files out of an estimated total of 100\n",
      "Uploading batch-data/34.csv\n",
      "Uploaded batch-data/34.csv, 29 files out of an estimated total of 100\n",
      "Uploading batch-data/35.csv\n",
      "Uploaded batch-data/35.csv, 30 files out of an estimated total of 100\n",
      "Uploading batch-data/36.csv\n",
      "Uploaded batch-data/36.csv, 31 files out of an estimated total of 100\n",
      "Uploading batch-data/37.csv\n",
      "Uploaded batch-data/37.csv, 32 files out of an estimated total of 100\n",
      "Uploading batch-data/38.csv\n",
      "Uploaded batch-data/38.csv, 33 files out of an estimated total of 100\n",
      "Uploading batch-data/39.csv\n",
      "Uploaded batch-data/39.csv, 34 files out of an estimated total of 100\n",
      "Uploading batch-data/4.csv\n",
      "Uploaded batch-data/4.csv, 35 files out of an estimated total of 100\n",
      "Uploading batch-data/40.csv\n",
      "Uploaded batch-data/40.csv, 36 files out of an estimated total of 100\n",
      "Uploading batch-data/41.csv\n",
      "Uploaded batch-data/41.csv, 37 files out of an estimated total of 100\n",
      "Uploading batch-data/42.csv\n",
      "Uploaded batch-data/42.csv, 38 files out of an estimated total of 100\n",
      "Uploading batch-data/44.csv\n",
      "Uploaded batch-data/44.csv, 39 files out of an estimated total of 100\n",
      "Uploading batch-data/45.csv\n",
      "Uploaded batch-data/45.csv, 40 files out of an estimated total of 100\n",
      "Uploading batch-data/46.csv\n",
      "Uploaded batch-data/46.csv, 41 files out of an estimated total of 100\n",
      "Uploading batch-data/47.csv\n",
      "Uploaded batch-data/47.csv, 42 files out of an estimated total of 100\n",
      "Uploading batch-data/48.csv\n",
      "Uploaded batch-data/48.csv, 43 files out of an estimated total of 100\n",
      "Uploading batch-data/49.csv\n",
      "Uploaded batch-data/49.csv, 44 files out of an estimated total of 100\n",
      "Uploading batch-data/5.csv\n",
      "Uploaded batch-data/5.csv, 45 files out of an estimated total of 100\n",
      "Uploading batch-data/50.csv\n",
      "Uploaded batch-data/50.csv, 46 files out of an estimated total of 100\n",
      "Uploading batch-data/51.csv\n",
      "Uploaded batch-data/51.csv, 47 files out of an estimated total of 100\n",
      "Uploading batch-data/52.csv\n",
      "Uploaded batch-data/52.csv, 48 files out of an estimated total of 100\n",
      "Uploading batch-data/53.csv\n",
      "Uploaded batch-data/53.csv, 49 files out of an estimated total of 100\n",
      "Uploading batch-data/54.csv\n",
      "Uploaded batch-data/54.csv, 50 files out of an estimated total of 100\n",
      "Uploading batch-data/55.csv\n",
      "Uploaded batch-data/55.csv, 51 files out of an estimated total of 100\n",
      "Uploading batch-data/56.csv\n",
      "Uploaded batch-data/56.csv, 52 files out of an estimated total of 100\n",
      "Uploading batch-data/43.csv\n",
      "Uploaded batch-data/43.csv, 53 files out of an estimated total of 100\n",
      "Uploading batch-data/57.csv\n",
      "Uploaded batch-data/57.csv, 54 files out of an estimated total of 100\n",
      "Uploading batch-data/58.csv\n",
      "Uploaded batch-data/58.csv, 55 files out of an estimated total of 100\n",
      "Uploading batch-data/59.csv\n",
      "Uploaded batch-data/59.csv, 56 files out of an estimated total of 100\n",
      "Uploading batch-data/6.csv\n",
      "Uploaded batch-data/6.csv, 57 files out of an estimated total of 100\n",
      "Uploading batch-data/60.csv\n",
      "Uploaded batch-data/60.csv, 58 files out of an estimated total of 100\n",
      "Uploading batch-data/61.csv\n",
      "Uploaded batch-data/61.csv, 59 files out of an estimated total of 100\n",
      "Uploading batch-data/62.csv\n",
      "Uploaded batch-data/62.csv, 60 files out of an estimated total of 100\n",
      "Uploading batch-data/63.csv\n",
      "Uploaded batch-data/63.csv, 61 files out of an estimated total of 100\n",
      "Uploading batch-data/64.csv\n",
      "Uploaded batch-data/64.csv, 62 files out of an estimated total of 100\n",
      "Uploading batch-data/65.csv\n",
      "Uploaded batch-data/65.csv, 63 files out of an estimated total of 100\n",
      "Uploading batch-data/66.csv\n",
      "Uploaded batch-data/66.csv, 64 files out of an estimated total of 100\n",
      "Uploading batch-data/67.csv\n",
      "Uploaded batch-data/67.csv, 65 files out of an estimated total of 100\n",
      "Uploading batch-data/68.csv\n",
      "Uploaded batch-data/68.csv, 66 files out of an estimated total of 100\n",
      "Uploading batch-data/69.csv\n",
      "Uploaded batch-data/69.csv, 67 files out of an estimated total of 100\n",
      "Uploading batch-data/7.csv\n",
      "Uploaded batch-data/7.csv, 68 files out of an estimated total of 100\n",
      "Uploading batch-data/70.csv\n",
      "Uploaded batch-data/70.csv, 69 files out of an estimated total of 100\n",
      "Uploading batch-data/71.csv\n",
      "Uploaded batch-data/71.csv, 70 files out of an estimated total of 100\n",
      "Uploading batch-data/72.csv\n",
      "Uploaded batch-data/72.csv, 71 files out of an estimated total of 100\n",
      "Uploading batch-data/73.csv\n",
      "Uploaded batch-data/73.csv, 72 files out of an estimated total of 100\n",
      "Uploading batch-data/74.csv\n",
      "Uploaded batch-data/74.csv, 73 files out of an estimated total of 100\n",
      "Uploading batch-data/75.csv\n",
      "Uploaded batch-data/75.csv, 74 files out of an estimated total of 100\n",
      "Uploading batch-data/76.csv\n",
      "Uploaded batch-data/76.csv, 75 files out of an estimated total of 100\n",
      "Uploading batch-data/77.csv\n",
      "Uploaded batch-data/77.csv, 76 files out of an estimated total of 100\n",
      "Uploading batch-data/78.csv\n",
      "Uploaded batch-data/78.csv, 77 files out of an estimated total of 100\n",
      "Uploading batch-data/79.csv\n",
      "Uploaded batch-data/79.csv, 78 files out of an estimated total of 100\n",
      "Uploading batch-data/8.csv\n",
      "Uploaded batch-data/8.csv, 79 files out of an estimated total of 100\n",
      "Uploading batch-data/80.csv\n",
      "Uploaded batch-data/80.csv, 80 files out of an estimated total of 100\n",
      "Uploading batch-data/81.csv\n",
      "Uploaded batch-data/81.csv, 81 files out of an estimated total of 100\n",
      "Uploading batch-data/82.csv\n",
      "Uploaded batch-data/82.csv, 82 files out of an estimated total of 100\n",
      "Uploading batch-data/83.csv\n",
      "Uploaded batch-data/83.csv, 83 files out of an estimated total of 100\n",
      "Uploading batch-data/84.csv\n",
      "Uploaded batch-data/84.csv, 84 files out of an estimated total of 100\n",
      "Uploading batch-data/85.csv\n",
      "Uploaded batch-data/85.csv, 85 files out of an estimated total of 100\n",
      "Uploading batch-data/86.csv\n",
      "Uploaded batch-data/86.csv, 86 files out of an estimated total of 100\n",
      "Uploading batch-data/87.csv\n",
      "Uploaded batch-data/87.csv, 87 files out of an estimated total of 100\n",
      "Uploading batch-data/88.csv\n",
      "Uploaded batch-data/88.csv, 88 files out of an estimated total of 100\n",
      "Uploading batch-data/89.csv\n",
      "Uploaded batch-data/89.csv, 89 files out of an estimated total of 100\n",
      "Uploading batch-data/9.csv\n",
      "Uploaded batch-data/9.csv, 90 files out of an estimated total of 100\n",
      "Uploading batch-data/90.csv\n",
      "Uploaded batch-data/90.csv, 91 files out of an estimated total of 100\n",
      "Uploading batch-data/91.csv\n",
      "Uploaded batch-data/91.csv, 92 files out of an estimated total of 100\n",
      "Uploading batch-data/92.csv\n",
      "Uploaded batch-data/92.csv, 93 files out of an estimated total of 100\n",
      "Uploading batch-data/93.csv\n",
      "Uploaded batch-data/93.csv, 94 files out of an estimated total of 100\n",
      "Uploading batch-data/94.csv\n",
      "Uploaded batch-data/94.csv, 95 files out of an estimated total of 100\n",
      "Uploading batch-data/95.csv\n",
      "Uploaded batch-data/95.csv, 96 files out of an estimated total of 100\n",
      "Uploading batch-data/96.csv\n",
      "Uploaded batch-data/96.csv, 97 files out of an estimated total of 100\n",
      "Uploading batch-data/97.csv\n",
      "Uploaded batch-data/97.csv, 98 files out of an estimated total of 100\n",
      "Uploading batch-data/98.csv\n",
      "Uploaded batch-data/98.csv, 99 files out of an estimated total of 100\n",
      "Uploading batch-data/99.csv\n",
      "Uploaded batch-data/99.csv, 100 files out of an estimated total of 100\n",
      "Uploaded 100 files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set default data store\n",
    "ws.set_default_datastore('workspaceblobstore')\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)\n",
    "\n",
    "# Load the diabetes data\n",
    "diabetes = pd.read_csv('data/diabetes2.csv')\n",
    "# Get a 100-item sample of the feature columns (not the diabetic label)\n",
    "sample = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].sample(n=100).values\n",
    "\n",
    "# Create a folder\n",
    "batch_folder = './batch-data'\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "print(\"Folder created!\")\n",
    "\n",
    "# Save each sample as a separate file\n",
    "print(\"Saving files...\")\n",
    "for i in range(100):\n",
    "    fname = str(i+1) + '.csv'\n",
    "    sample[i].tofile(os.path.join(batch_folder, fname), sep=\",\")\n",
    "print(\"files saved!\")\n",
    "\n",
    "# Upload the files to the default datastore\n",
    "print(\"Uploading files to datastore...\")\n",
    "default_ds = ws.get_default_datastore()\n",
    "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\n",
    "\n",
    "# Register a dataset for the input data\n",
    "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\n",
    "try:\n",
    "    batch_data_set = batch_data_set.register(workspace=ws, \n",
    "                                             name='batch-data',\n",
    "                                             description='batch data',\n",
    "                                             create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Compute\n",
    "\n",
    "We'll need a compute context for the pipeline, so we'll use the Azure ML compute cluster you used in the previous exercises (it will be created if it doesn't already exist).\n",
    "\n",
    "> **Important**: Change *your-compute-cluster* to the name of your compute cluster in the code below before running it! Cluster names must be globally unique names between 2 to 16 characters in length. Valid characters are letters, digits, and the - character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"B-ML3-CmpCluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        inference_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        inference_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline for Batch Inferencing\n",
    "\n",
    "Now we're ready to define the pipeline we'll use for batch inferencing. Our pipeline will need Python code to perform the batch inferencing, so let's create a folder where we can keep all the files used by the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = 'batch_pipeline'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a Python script to do the actual work, and save it in the pipeline folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch_pipeline/batch_diabetes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/batch_diabetes.py\n",
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "\n",
    "\n",
    "def init():\n",
    "    # Runs when the pipeline step is initialized\n",
    "    global model\n",
    "\n",
    "    # load the model\n",
    "    model_path = Model.get_model_path('diabetes_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    resultList = []\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        # Read the comma-delimited data into an array\n",
    "        data = np.genfromtxt(f, delimiter=',')\n",
    "        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n",
    "        prediction = model.predict(data.reshape(1, -1))\n",
    "        # Append prediction to results\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define a run context that includes the dependencies required by the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration ready.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "# Add dependencies required by the model\n",
    "# For scikit-learn models, you need scikit-learn\n",
    "# For parallel pipeline steps, you need azureml-core and azureml-dataprep[fuse]\n",
    "cd = CondaDependencies.create(pip_packages=['scikit-learn','azureml-defaults','azureml-core','azureml-dataprep[fuse]'])\n",
    "\n",
    "batch_env = Environment(name='batch_environment')\n",
    "batch_env.python.conda_dependencies = cd\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "print('Configuration ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to use a pipeline to run the batch prediction script, generate predictions from the input data, and save the results as a text file in the output folder. To do this, you can use a **ParallelRunStep**, which enables the batch data to be processed in parallel and the results collated in a single output file named *parallel_run_step.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "output_dir = PipelineData(name='inferences', \n",
    "                          datastore=default_ds, \n",
    "                          output_path_on_compute='diabetes/results')\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=experiment_folder,\n",
    "    entry_script=\"batch_diabetes.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=inference_cluster,\n",
    "    node_count=2)\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score-diabetes',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('diabetes_batch')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print('Steps defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to put the step into a pipeline, and run it.\n",
    "\n",
    "> **Note**: This may take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step batch-score-diabetes [edae6512][094e1ea6-cedc-413e-bbb0-e086cffa290f], (This step will run and generate new outputs)\n",
      "Created data reference diabetes_batch_0 for StepId [60bb3667][9bc5c899-fbc0-4cb8-8096-301135903e14], (Consumers of this data will generate new runs.)\n",
      "Submitted PipelineRun e6be245f-dddf-4a12-bc32-114dd538dc57\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/batch_prediction_pipeline/runs/e6be245f-dddf-4a12-bc32-114dd538dc57?wsid=/subscriptions/94670d96-c488-4d89-a5e8-ea4fef48bf6a/resourcegroups/MS_SolutionCenter/workspaces/BSM_MLWorkspace3\n",
      "PipelineRunId: e6be245f-dddf-4a12-bc32-114dd538dc57\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/batch_prediction_pipeline/runs/e6be245f-dddf-4a12-bc32-114dd538dc57?wsid=/subscriptions/94670d96-c488-4d89-a5e8-ea4fef48bf6a/resourcegroups/MS_SolutionCenter/workspaces/BSM_MLWorkspace3\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: c7b895d2-507f-47b0-9259-7f8af1d16b16\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/batch_prediction_pipeline/runs/c7b895d2-507f-47b0-9259-7f8af1d16b16?wsid=/subscriptions/94670d96-c488-4d89-a5e8-ea4fef48bf6a/resourcegroups/MS_SolutionCenter/workspaces/BSM_MLWorkspace3\n",
      "StepRun( batch-score-diabetes ) Status: NotStarted\n",
      "StepRun( batch-score-diabetes ) Status: Queued\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "2020/09/04 00:39:12 Downloading source code...\n",
      "StepRun( batch-score-diabetes ) Status: Running\n",
      "2020/09/04 00:39:13 Finished downloading source code\n",
      "2020/09/04 00:39:14 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2020/09/04 00:39:14 Successfully set up Docker network: acb_default_network\n",
      "2020/09/04 00:39:14 Setting up Docker configuration...\n",
      "2020/09/04 00:39:15 Successfully set up Docker configuration\n",
      "2020/09/04 00:39:15 Logging in to registry: 57eddf13faae4a7292cbe457f54822f1.azurecr.io\n",
      "2020/09/04 00:39:16 Successfully logged into 57eddf13faae4a7292cbe457f54822f1.azurecr.io\n",
      "2020/09/04 00:39:16 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2020/09/04 00:39:16 Scanning for dependencies...\n",
      "2020/09/04 00:39:17 Successfully scanned dependencies\n",
      "2020/09/04 00:39:17 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  60.93kB\n",
      "\n",
      "Step 1/15 : FROM mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1@sha256:8cee6f674276dddb23068d2710da7f7f95b119412cc482675ac79ba45a4acf99\n",
      "sha256:8cee6f674276dddb23068d2710da7f7f95b119412cc482675ac79ba45a4acf99: Pulling from azureml/intelmpi2018.3-ubuntu16.04\n",
      "8e097b52bfb8: Pulling fs layer\n",
      "a613a9b4553c: Pulling fs layer\n",
      "acc000f01536: Pulling fs layer\n",
      "73eef93b7466: Pulling fs layer\n",
      "d5a54c1fb97f: Pulling fs layer\n",
      "1536f6ca931b: Pulling fs layer\n",
      "d7b631d130cb: Pulling fs layer\n",
      "75ffe8dfb222: Pulling fs layer\n",
      "86b4bf2f8d5f: Pulling fs layer\n",
      "5335952fa8d3: Pulling fs layer\n",
      "96fa3cc6fe10: Pulling fs layer\n",
      "e428dd9daa94: Pulling fs layer\n",
      "73eef93b7466: Waiting\n",
      "d5a54c1fb97f: Waiting\n",
      "1536f6ca931b: Waiting\n",
      "d7b631d130cb: Waiting\n",
      "75ffe8dfb222: Waiting\n",
      "86b4bf2f8d5f: Waiting\n",
      "5335952fa8d3: Waiting\n",
      "96fa3cc6fe10: Waiting\n",
      "e428dd9daa94: Waiting\n",
      "acc000f01536: Verifying Checksum\n",
      "acc000f01536: Download complete\n",
      "a613a9b4553c: Download complete\n",
      "73eef93b7466: Verifying Checksum\n",
      "73eef93b7466: Download complete\n",
      "8e097b52bfb8: Verifying Checksum\n",
      "8e097b52bfb8: Download complete\n",
      "1536f6ca931b: Verifying Checksum\n",
      "1536f6ca931b: Download complete\n",
      "d7b631d130cb: Verifying Checksum\n",
      "d7b631d130cb: Download complete\n",
      "d5a54c1fb97f: Verifying Checksum\n",
      "d5a54c1fb97f: Download complete\n",
      "75ffe8dfb222: Verifying Checksum\n",
      "75ffe8dfb222: Download complete\n",
      "5335952fa8d3: Verifying Checksum\n",
      "5335952fa8d3: Download complete\n",
      "96fa3cc6fe10: Verifying Checksum\n",
      "96fa3cc6fe10: Download complete\n",
      "e428dd9daa94: Verifying Checksum\n",
      "e428dd9daa94: Download complete\n",
      "86b4bf2f8d5f: Verifying Checksum\n",
      "86b4bf2f8d5f: Download complete\n",
      "8e097b52bfb8: Pull complete\n",
      "a613a9b4553c: Pull complete\n",
      "acc000f01536: Pull complete\n",
      "73eef93b7466: Pull complete\n",
      "d5a54c1fb97f: Pull complete\n",
      "1536f6ca931b: Pull complete\n",
      "d7b631d130cb: Pull complete\n",
      "aece08fd27fc: Pushed\n",
      "2817caf0a082: Pushed\n",
      "dcc0cc99372e: Pushed\n",
      "8dab94e6d05c: Pushed\n",
      "41a253a417e6: Pushed\n",
      "87c128261339: Pushed\n",
      "5e1805eb9eb5: Pushed\n",
      "e06660e80cf4: Pushed\n",
      "4caea5ef1f0b: Pushed\n",
      "\n",
      "d3ee41d70058: Pushed\n",
      "latest: digest: sha256:d04a2de47c4d981295a305d96e7afe93f5c38d0b663139da330011ecbacf2a4b size: 4095\n",
      "2020/09/04 00:44:00 Successfully pushed image: 57eddf13faae4a7292cbe457f54822f1.azurecr.io/azureml/azureml_7d268cc88aa2d5eaef04c4b16f0396b2:latest\n",
      "2020/09/04 00:44:00 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 174.900194)\n",
      "2020/09/04 00:44:00 Populating digests for step ID: acb_step_0...\n",
      "2020/09/04 00:44:01 Successfully populated digests for step ID: acb_step_0\n",
      "2020/09/04 00:44:01 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 109.206544)\n",
      "2020/09/04 00:44:01 The following dependencies were found:\n",
      "2020/09/04 00:44:01 \n",
      "- image:\n",
      "    registry: 57eddf13faae4a7292cbe457f54822f1.azurecr.io\n",
      "    repository: azureml/azureml_7d268cc88aa2d5eaef04c4b16f0396b2\n",
      "    tag: latest\n",
      "    digest: sha256:d04a2de47c4d981295a305d96e7afe93f5c38d0b663139da330011ecbacf2a4b\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/intelmpi2018.3-ubuntu16.04\n",
      "    tag: 20200821.v1\n",
      "    digest: sha256:8cee6f674276dddb23068d2710da7f7f95b119412cc482675ac79ba45a4acf99\n",
      "  git: {}\n",
      "\n",
      "Run ID: ch4 was successful after 4m51s\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "pipeline_run = Experiment(ws, 'batch_prediction_pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the pipeline has finished running, the resulting predictions will have been saved in the outputs of the experiment associated with the first (and only) step in the pipeline. You can retrieve it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>91.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>92.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>93.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>94.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>95.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>96.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>97.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>98.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>99.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      File  Prediction\n",
       "0   18.csv           0\n",
       "1   19.csv           1\n",
       "2    2.csv           0\n",
       "3   20.csv           0\n",
       "4   21.csv           1\n",
       "5   27.csv           0\n",
       "6   28.csv           0\n",
       "7   29.csv           1\n",
       "8    3.csv           0\n",
       "9   30.csv           0\n",
       "10  90.csv           0\n",
       "11  91.csv           0\n",
       "12  92.csv           0\n",
       "13  93.csv           0\n",
       "14  94.csv           0\n",
       "15  95.csv           0\n",
       "16  96.csv           1\n",
       "17  97.csv           1\n",
       "18  98.csv           0\n",
       "19  99.csv           0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
    "\n",
    "prediction_run = next(pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data('inferences')\n",
    "prediction_output.download(local_path='diabetes-results')\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk('diabetes-results'):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# cleanup output format\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "\n",
    "# Display the first 20 results\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish the Pipeline and use its REST Interface\n",
    "\n",
    "Now that you have a working pipeline for batch inferencing, you can publish it and use a REST endpoint to run it from an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>Diabetes_Parallel_Batch_Pipeline</td><td><a href=\"https://ml.azure.com/pipelines/f603e6cd-dd76-4c03-9559-29e61184de1a?wsid=/subscriptions/94670d96-c488-4d89-a5e8-ea4fef48bf6a/resourcegroups/MS_SolutionCenter/workspaces/BSM_MLWorkspace3\" target=\"_blank\" rel=\"noopener\">f603e6cd-dd76-4c03-9559-29e61184de1a</a></td><td>Active</td><td><a href=\"https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/94670d96-c488-4d89-a5e8-ea4fef48bf6a/resourceGroups/MS_SolutionCenter/providers/Microsoft.MachineLearningServices/workspaces/BSM_MLWorkspace3/PipelineRuns/PipelineSubmit/f603e6cd-dd76-4c03-9559-29e61184de1a\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: Diabetes_Parallel_Batch_Pipeline,\n",
       "Id: f603e6cd-dd76-4c03-9559-29e61184de1a,\n",
       "Status: Active,\n",
       "Endpoint: https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/94670d96-c488-4d89-a5e8-ea4fef48bf6a/resourceGroups/MS_SolutionCenter/providers/Microsoft.MachineLearningServices/workspaces/BSM_MLWorkspace3/PipelineRuns/PipelineSubmit/f603e6cd-dd76-4c03-9559-29e61184de1a)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name='Diabetes_Parallel_Batch_Pipeline', description='Batch scoring of diabetes data', version='1.0')\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the published pipeline has an endpoint, which you can see in the Azure portal. You can also find it as a property of the published pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/94670d96-c488-4d89-a5e8-ea4fef48bf6a/resourceGroups/MS_SolutionCenter/providers/Microsoft.MachineLearningServices/workspaces/BSM_MLWorkspace3/PipelineRuns/PipelineSubmit/f603e6cd-dd76-4c03-9559-29e61184de1a\n"
     ]
    }
   ],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the endpoint, client applications need to make a REST call over HTTP. This request must be authenticated, so an authorization header is required. To test this out, we'll use the authorization header from your current connection to your Azure workspace, which you can get using the following code:\n",
    "\n",
    "> **Note**: A real application would require a service principal with which to be authenticated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication header ready.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "print('Authentication header ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to call the REST interface. The pipeline runs asynchronously, so we'll get an identifier back, which we can use to track the pipeline experiment as it runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5bd0486a-d7e1-427c-b200-410d5e7807cf'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": \"Batch_Pipeline_via_REST\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the run ID, we can use the **RunDetails** widget to view the experiment as it runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19648d645e6845119694f791534be16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/Batch_Pipeline_via_REST/runs/5bd0486a-d7e1-427c-b200-410d5e7807cf?wsid=/subscriptions/94670d96-c488-4d89-a5e8-ea4fef48bf6a/resourcegroups/MS_SolutionCenter/workspaces/BSM_MLWorkspace3\", \"run_id\": \"5bd0486a-d7e1-427c-b200-410d5e7807cf\", \"run_properties\": {\"run_id\": \"5bd0486a-d7e1-427c-b200-410d5e7807cf\", \"created_utc\": \"2020-09-04T06:04:48.551247Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"Unavailable\", \"runType\": \"HTTP\", \"azureml.parameters\": \"{}\", \"azureml.pipelineid\": \"f603e6cd-dd76-4c03-9559-29e61184de1a\"}, \"tags\": {\"azureml.pipelineid\": \"f603e6cd-dd76-4c03-9559-29e61184de1a\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2020-09-04T06:12:11.071457Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://bsmmlworkspace6998297976.blob.core.windows.net/azureml/ExperimentRun/dcid.5bd0486a-d7e1-427c-b200-410d5e7807cf/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=BpbdiYRkLfneCxOPhQMrP8qyKP%2FTDNGV8C6p8S4oyFU%3D&st=2020-09-04T10%3A57%3A56Z&se=2020-09-04T19%3A07%3A56Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://bsmmlworkspace6998297976.blob.core.windows.net/azureml/ExperimentRun/dcid.5bd0486a-d7e1-427c-b200-410d5e7807cf/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=ZsREl6GdmeZbOTDRdZ2YG3d0iRNG1cLrXB%2F1rtoggCA%3D&st=2020-09-04T10%3A57%3A56Z&se=2020-09-04T19%3A07%3A56Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://bsmmlworkspace6998297976.blob.core.windows.net/azureml/ExperimentRun/dcid.5bd0486a-d7e1-427c-b200-410d5e7807cf/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=%2F08ArQMf5ILkVruib0AUhVXGkWzWtDPSVDCZT60Fdi8%3D&st=2020-09-04T10%3A57%3A56Z&se=2020-09-04T19%3A07%3A56Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:07:22\"}, \"child_runs\": [{\"run_id\": \"8f8ce208-da5f-486d-891d-dbbce06bf258\", \"name\": \"batch-score-diabetes\", \"status\": \"Finished\", \"start_time\": \"2020-09-04T06:09:47.42619Z\", \"created_time\": \"2020-09-04T06:05:04.891652Z\", \"end_time\": \"2020-09-04T06:12:06.069624Z\", \"duration\": \"0:07:01\", \"run_number\": 2, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-09-04T06:05:04.891652Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-09-04 06:05:04Z] Submitting 1 runs, first five are: edae6512:8f8ce208-da5f-486d-891d-dbbce06bf258\\n[2020-09-04 06:12:10Z] Completing processing run id 8f8ce208-da5f-486d-891d-dbbce06bf258.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {\"8d22f150\": {\"node_id\": \"8d22f150\", \"name\": \"batch-data\"}, \"60bb3667\": {\"node_id\": \"60bb3667\", \"name\": \"diabetes_batch_0\"}}, \"module_nodes\": {\"edae6512\": {\"node_id\": \"edae6512\", \"name\": \"batch-score-diabetes\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"8f8ce208-da5f-486d-891d-dbbce06bf258\"}}, \"edges\": [{\"source_node_id\": \"8d22f150\", \"source_node_name\": \"batch-data\", \"source_name\": \"data\", \"target_name\": \"diabetes_batch\", \"dst_node_id\": \"edae6512\", \"dst_node_name\": \"batch-score-diabetes\"}, {\"source_node_id\": \"60bb3667\", \"source_node_name\": \"diabetes_batch_0\", \"source_name\": \"data\", \"target_name\": \"diabetes_batch\", \"dst_node_id\": \"edae6512\", \"dst_node_name\": \"batch-score-diabetes\"}], \"child_runs\": [{\"run_id\": \"8f8ce208-da5f-486d-891d-dbbce06bf258\", \"name\": \"batch-score-diabetes\", \"status\": \"Finished\", \"start_time\": \"2020-09-04T06:09:47.42619Z\", \"created_time\": \"2020-09-04T06:05:04.891652Z\", \"end_time\": \"2020-09-04T06:12:06.069624Z\", \"duration\": \"0:07:01\", \"run_number\": 2, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2020-09-04T06:05:04.891652Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.13.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments[\"Batch_Pipeline_via_REST\"], run_id)\n",
    "RunDetails(published_pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: If the widget displays the message `[\"AttributeError: 'NoneType' object has no attribute 'id'\\n\"]`, you can safely ignore it!\n",
    "\n",
    "As before, the results are in the output of the first pipeline step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>91.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>92.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>93.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>94.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>81.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>82.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>83.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>84.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>85.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      File  Prediction\n",
       "0   95.csv           0\n",
       "1   96.csv           1\n",
       "2   97.csv           1\n",
       "3   98.csv           0\n",
       "4   99.csv           0\n",
       "5   36.csv           0\n",
       "6   37.csv           1\n",
       "7   38.csv           0\n",
       "8   39.csv           1\n",
       "9    4.csv           0\n",
       "10  90.csv           0\n",
       "11  91.csv           0\n",
       "12  92.csv           0\n",
       "13  93.csv           0\n",
       "14  94.csv           0\n",
       "15  81.csv           0\n",
       "16  82.csv           0\n",
       "17  83.csv           0\n",
       "18  84.csv           1\n",
       "19  85.csv           0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"diabetes-results\", ignore_errors=True)\n",
    "\n",
    "prediction_run = next(published_pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data(\"inferences\")\n",
    "prediction_output.download(local_path=\"diabetes-results\")\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"diabetes-results\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# cleanup output format\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
    "df.columns = [\"File\", \"Prediction\"]\n",
    "\n",
    "# Display the first 20 results\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a pipeline that can be used to batch process daily patient data.\n",
    "\n",
    "**More Information**: For more details about using pipelines for batch inferencing, see the [How to Run Batch Predictions](https://docs.microsoft.com/azure/machine-learning/how-to-run-batch-predictions) in the Azure Machine Learning documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
